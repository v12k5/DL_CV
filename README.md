ğŸ¤– Lightweight Contrastive Pretraining for Visual RL
Train your visual navigation agent with less data and less compute! This project provides a resource-efficient framework for goal-conditioned navigation in MiniWorld using a powerful two-stage learning process.

ğŸ’¡ The Core Idea
Instead of training a giant model from scratch on millions of frames, we first teach a small "visual brain" (a CNN encoder) to understand what different viewpoints of the world look like. We do this with contrastive learning, showing it pairs of images and asking, "Are these two views of the same place?"

Once this brain is pretrained, we freeze it and use its powerful representations to train a navigation agent with Reinforcement Learning (RL) much more quickly and efficiently. The agent's reward is simple: make what I see now look more like my goal.

ğŸ—ï¸ Visual Architecture
Our framework is split into two distinct stages: Pretraining and RL Training.

Stage 1: Contrastive Pretraining (Unsupervised)
The agent explores the environment randomly, like a baby crawling around a room.

It collects thousands of unlabeled image frames.

A SimCLR-style model learns to create compact representations (embeddings) of these frames. The goal is to map similar-looking frames to nearby points in the embedding space.

Outcome: A smart, frozen Visual Encoder that understands visual similarity.

Code snippet

graph TD
    A[MiniWorld Environment] -->|Random Exploration| B(Unlabeled Image Frames);
    B --> C{SimCLR Contrastive Learning};
    C -->|Augmented Views| C;
    C --> D[ğŸ§  Frozen Visual Encoder];
Stage 2: Goal-Conditioned RL Training
The frozen encoder is now used to guide the RL agent (PPO).

For each step, the agent gets a current observation and a goal image.

Both images are passed through the encoder to get their embeddings.

The PPO policy receives both embeddings and decides on an action (e.g., turn left, move forward).

The reward is calculated based on how much closer the current view's embedding is to the goal's embedding.

Code snippet

graph TD
    subgraph RL Loop
        E(Current Observation) --> F[ğŸ§  Frozen Encoder];
        G(Goal Image) --> F;
        F --> H{Embedding Similarity Reward};
        F --> I[ğŸ¤– PPO Policy];
        H --> I;
        I --> J(Action);
        J --> K[MiniWorld Environment];
        K --> E;
    end
âœ¨ Key Features
ğŸ’¡ Lightweight Design: A compact CNN encoder with only ~1 million parameters.

ğŸ“‰ Sample Efficient: Drastically reduces the number of labeled interactions needed for RL training.

ğŸ’» Resource Friendly: Train everything on a single consumer-grade GPU.

ğŸ§© Modular Pipeline: Easily swap out encoders, policies, or environments.

ğŸŒ Goal Generalization: Learns viewpoint-invariant features, helping the agent navigate to goals from novel starting positions.

âœ… Complete & Reproducible: Full implementation from data collection to final evaluation.

ğŸš€ Quick Start
1. Installation
Bash

# Clone the repository
git clone <your-repo-url>
cd contrastive-visual-rl

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install all dependencies
pip install -r requirements.txt
2. Run the Full Pipeline (Recommended)
Execute the entire processâ€”data collection, pretraining, and RL trainingâ€”with a single command.

Bash

python main.py --mode full --num_episodes 100 --contrastive_epochs 50 --rl_episodes 1000
3. Step-by-Step Execution
Alternatively, run each stage individually.

Step 1: Collect Exploration Data ğŸ“·
Bash

# Gathers unlabeled frames from random exploration
python main.py --mode collect --num_episodes 100
Step 2: Pretrain Contrastive Encoder ğŸ§ 
Bash

# Trains the encoder with SimCLR-style contrastive learning
python main.py --mode pretrain --contrastive_epochs 50 --batch_size 64
Step 3: Train PPO Policy ğŸ¤–
Bash

# Trains the navigation policy using the frozen encoder and similarity rewards
python main.py --mode train_rl --rl_episodes 1000 --max_steps 200
Step 4: Evaluate the Trained Agent ğŸ†
Bash

# Evaluates the final policy and saves video recordings
python main.py --mode evaluate --eval_episodes 20 --save_video
ğŸ“ Project Structure
.
â”œâ”€â”€ ğŸ“œ main.py                  # Main pipeline script to run all stages
â”œâ”€â”€ ğŸ“œ contrastive_encoder.py    # CNN encoder architecture & SimCLR loss
â”œâ”€â”€ ğŸ“œ data_collection.py        # Logic for random exploration and data saving
â”œâ”€â”€ ğŸ“œ train_contrastive.py      # Script for Stage 1: Pretraining
â”œâ”€â”€ ğŸ“œ ppo_policy.py              # PPO agent and buffer implementation
â”œâ”€â”€ ğŸ“œ goal_env_wrapper.py      # Gym wrapper for goal-conditioning & rewards
â”œâ”€â”€ ğŸ“œ train_ppo.py               # Script for Stage 2: RL Training
â”œâ”€â”€ ğŸ“œ evaluate.py                # Evaluation, visualization & video saving
â”œâ”€â”€ ğŸ“œ requirements.txt           # Project dependencies
â”œâ”€â”€ ğŸ“ data/                      # (Auto-created) Stores collected frames
â”œâ”€â”€ ğŸ“ models/                    # (Auto-created) Stores trained encoder & policy
â””â”€â”€ ğŸ“ videos/                    # (Auto-created) Stores evaluation videos
ğŸ“Š Expected Results
After full training, you should observe:

Contrastive Loss: A steady decrease to a low value (e.g., ~0.5-1.0), indicating the encoder is learning meaningful representations.

RL Performance: A clear upward trend in the mean reward and success rate during training.

Sample Evaluation Output:
EVALUATION SUMMARY
==================================================
Episodes: 20
Mean Reward: 15.34 Â± 8.21
Mean Length: 87.50 Â± 35.12
Success Rate: 75.0%
Mean Final Distance: 0.087 Â± 0.112
==================================================
ğŸ® Supported Environments
Easily switch between different MiniWorld environments.

MiniWorld-Hallway-v0 (default)

MiniWorld-OneRoom-v0

MiniWorld-TMaze-v0

MiniWorld-FourRooms-v0

Example:

Bash

python main.py --mode full --env_name MiniWorld-FourRooms-v0
ğŸ› ï¸ Troubleshooting & Extending
Common Issues
Out of Memory? Reduce --batch_size, collect fewer frames with --num_episodes, or use the CPU with --device cpu.

Low Success Rate? Increase --contrastive_epochs for better visual representations, collect more data, or simplify the environment.

Extending the Project
Want to add your own spin? The code is modular!

New Augmentations: Edit SimCLRAugmentation in contrastive_encoder.py.

Different Encoders: Modify the ContrastiveEncoder class to use architectures like ResNet or add attention.

Custom Rewards: Change the reward logic in GoalConditionedWrapper.step() in goal_env_wrapper.py.